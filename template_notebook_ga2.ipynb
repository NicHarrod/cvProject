{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KUL H02A5a Computer Vision: Group Assignment 2\n",
    "---------------------------------------------------------------\n",
    "Student numbers: <span style=\"color:red\">r1, r2, r3, r4, r5</span>. (fill in your student numbers!)\n",
    "\n",
    "In this group assignment your team will delve into some deep learning applications for computer vision. The assignment will be delivered in the same groups from *Group assignment 1* and you start from this template notebook. The notebook you submit for grading is the last notebook pinned as default and submitted to the [Kaggle competition](https://www.kaggle.com/t/90a3b6380ecb4700857b9e07a44ca41b) prior to the deadline on **Tuesday 20 May 23:59**. Closely follow [these instructions](https://github.com/gourie/kaggle_inclass) for joining the competition, sharing your notebook with the TAs and making a valid notebook submission to the competition. A notebook submission not only produces a *submission.csv* file that is used to calculate your competition score, it also runs the entire notebook and saves its output as if it were a report. This way it becomes an all-in-one-place document for the TAs to review. As such, please make sure that your final submission notebook is self-contained and fully documented (e.g. provide strong arguments for the design choices that you make). Most likely, this notebook format is not appropriate to run all your experiments at submission time (e.g. the training of CNNs is a memory hungry and time consuming process; due to limited Kaggle resources). It can be a good idea to distribute your code otherwise and only summarize your findings, together with your final predictions, in the submission notebook. For example, you can substitute experiments with some text and figures that you have produced \"offline\" (e.g. learning curves and results on your internal validation set or even the test set for different architectures, pre-processing pipelines, etc). We advise you to first go through the PDF of this assignment entirely before you really start. Then, it can be a good idea to go through this notebook and use it as your first notebook submission to the competition. You can make use of the *Group assignment 2* forum/discussion board on Toledo if you have any questions. Good luck and have fun!\n",
    "\n",
    "---------------------------------------------------------------\n",
    "NOTES:\n",
    "* This notebook is just a template. Please keep the five main sections, but feel free to adjust further in any way you please!\n",
    "* Clearly indicate the improvements that you make! You can for instance use subsections like: *3.1. Improvement: applying loss function f instead of g*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overview\n",
    "This assignment consists of *three main parts* for which we expect you to provide code and extensive documentation in the notebook:\n",
    "* Image classification (Sect. 2)\n",
    "* Semantic segmentation (Sect. 3)\n",
    "* Adversarial attacks (Sect. 4)\n",
    "\n",
    "In the first part, you will train an end-to-end neural network for image classification. In the second part, you will do the same for semantic segmentation. For these two tasks we expect you to put a significant effort into optimizing performance and as such competing with fellow students via the Kaggle competition. In the third part, you will try to find and exploit the weaknesses of your classification and/or segmentation network. For the latter there is no competition format, but we do expect you to put significant effort in achieving good performance on the self-posed goal for that part. Finally, we ask you to reflect and produce an overall discussion with links to the lectures and \"real world\" computer vision (Sect. 5). It is important to note that only a small part of the grade will reflect the actual performance of your networks. However, we do expect all things to work! In general, we will evaluate the correctness of your approach and your understanding of what you have done that you demonstrate in the descriptions and discussions in the final notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Deep learning resources\n",
    "If you did not yet explore this in *Group assignment 1 (Sect. 2)*, we recommend using the TensorFlow and/or Keras library for building deep learning models. You can find a nice crash course [here](https://colab.research.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-07T17:32:16.166Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms.functional import resize\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 PASCAL VOC 2009\n",
    "For this project you will be using the [PASCAL VOC 2009](http://host.robots.ox.ac.uk/pascal/VOC/voc2009/index.html) dataset. This dataset consists of colour images of various scenes with different object classes (e.g. animal: *bird, cat, ...*; vehicle: *aeroplane, bicycle, ...*), totalling 20 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-07T17:32:16.166Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loading the training data\n",
    "train_df = pd.read_csv('/kaggle/input/kul-computer-vision-ga-2-2025/train/train_set.csv', index_col=\"Id\")\n",
    "# train_df =pd.read_csv('train/train_set.csv', index_col=\"Id\")\n",
    "labels = train_df.columns\n",
    "train_df[\"img\"] = [np.load('/kaggle/input/kul-computer-vision-ga-2-2025/train/img/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n",
    "train_df[\"seg\"] = [np.load('/kaggle/input/kul-computer-vision-ga-2-2025/train/seg/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n",
    "# train_df[\"img\"] = [np.load('train/img/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n",
    "# train_df[\"seg\"] = [np.load('train/seg/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n",
    "print(\"The training set contains {} examples.\".format(len(train_df)))\n",
    "\n",
    "# Show some examples\n",
    "fig, axs = plt.subplots(2, 20, figsize=(10 * 20, 10 * 2))\n",
    "for i, label in enumerate(labels):\n",
    "    df = train_df.loc[train_df[label] == 1]\n",
    "    axs[0, i].imshow(df.iloc[0][\"img\"], vmin=0, vmax=255)\n",
    "    axs[0, i].set_title(\"\\n\".join(label for label in labels if df.iloc[0][label] == 1), fontsize=40)\n",
    "    axs[0, i].axis(\"off\")\n",
    "    axs[1, i].imshow(df.iloc[0][\"seg\"], vmin=0, vmax=20)  # with the absolute color scale it will be clear that the arrays in the \"seg\" column are label maps (labels in [0, 20])\n",
    "    axs[1, i].axis(\"off\")\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# The training dataframe contains for each image 20 columns with the ground truth classification labels and 20 column with the ground truth segmentation maps for each class\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-07T17:32:16.166Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loading the test data\n",
    "test_df = pd.read_csv('/kaggle/input/kul-computer-vision-ga-2-2025/test/test_set.csv', index_col=\"Id\")\n",
    "test_df[\"img\"] = [np.load('/kaggle/input/kul-computer-vision-ga-2-2025/test/img/test_{}.npy'.format(idx)) for idx, _ in test_df.iterrows()]\n",
    "# test_df = pd.read_csv('test/test_set.csv', index_col=\"Id\")\n",
    "# test_df[\"img\"] = [np.load('test/img/test_{}.npy'.format(idx)) for idx, _ in test_df.iterrows()]\n",
    "test_df[\"seg\"] = [-1 * np.ones(img.shape[:2], dtype=np.int8) for img in test_df[\"img\"]]\n",
    "print(\"The test set contains {} examples.\".format(len(test_df)))\n",
    "\n",
    "# The test dataframe is similar to the training dataframe, but here the values are -1 --> your task is to fill in these as good as possible in Sect. 2 and Sect. 3; in Sect. 6 this dataframe is automatically transformed in the submission CSV!\n",
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Your Kaggle submission\n",
    "Your filled test dataframe (during Sect. 2 and Sect. 3) must be converted to a submission.csv with two rows per example (one for classification and one for segmentation) and with only a single prediction column (the multi-class/label predictions running length encoded). You don't need to edit this section. Just make sure to call this function at the right position in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-07T17:32:16.166Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _rle_encode(img):\n",
    "    \"\"\"\n",
    "    Kaggle requires RLE encoded predictions for computation of the Dice score (https://www.kaggle.com/lifa08/run-length-encode-and-decode)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img: np.ndarray - binary img array\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    rle: String - running length encoded version of img\n",
    "    \"\"\"\n",
    "    pixels = img.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    rle = ' '.join(str(x) for x in runs)\n",
    "    return rle\n",
    "\n",
    "def generate_submission(df):\n",
    "    \"\"\"\n",
    "    Make sure to call this function once after you completed Sect. 2 and Sect. 3! It transforms and writes your test dataframe into a submission.csv file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame - filled dataframe that needs to be converted\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    submission_df: pd.DataFrame - df in submission format.\n",
    "    \"\"\"\n",
    "    df_dict = {\"Id\": [], \"Predicted\": []}\n",
    "    for idx, _ in df.iterrows():\n",
    "        df_dict[\"Id\"].append(f\"{idx}_classification\")\n",
    "        df_dict[\"Predicted\"].append(_rle_encode(np.array(df.loc[idx, labels])))\n",
    "        df_dict[\"Id\"].append(f\"{idx}_segmentation\")\n",
    "        df_dict[\"Predicted\"].append(_rle_encode(np.array([df.loc[idx, \"seg\"] == j + 1 for j in range(len(labels))])))\n",
    "    \n",
    "    submission_df = pd.DataFrame(data=df_dict, dtype=str).set_index(\"Id\")\n",
    "    submission_df.to_csv(\"submission.csv\")\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Image classification\n",
    "The goal here is simple: implement a classification CNN and train it to recognise all 20 classes (and/or background) using the training set and compete on the test set (by filling in the classification columns in the test dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-07T17:32:16.166Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RandomClassificationModel:\n",
    "    \"\"\"\n",
    "    Random classification model: \n",
    "        - generates random labels for the inputs based on the class distribution observed during training\n",
    "        - assumes an input can have multiple labels\n",
    "    \"\"\"\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Adjusts the class ratio variable to the one observed in y. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: list of arrays - n x (height x width x 3)\n",
    "        y: list of arrays - n x (nb_classes)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        self.distribution = np.mean(y, axis=0)\n",
    "        print(\"Setting class distribution to:\\n{}\".format(\"\\n\".join(f\"{label}: {p}\" for label, p in zip(labels, self.distribution))))\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts for each input a label.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: list of arrays - n x (height x width x 3)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: list of arrays - n x (nb_classes)\n",
    "        \"\"\"\n",
    "        np.random.seed(0)\n",
    "        return [np.array([int(np.random.rand() < p) for p in self.distribution]) for _ in X]\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)\n",
    "    \n",
    "model = RandomClassificationModel()\n",
    "model.fit(train_df[\"img\"], train_df[labels])\n",
    "test_df.loc[:, labels] = model.predict(test_df[\"img\"])\n",
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Semantic segmentation\n",
    "The goal here is to implement a segmentation CNN that labels every pixel in the image as belonging to one of the 20 classes (and/or background). Use the training set to train your CNN and compete on the test set (by filling in the segmentation column in the test dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-07T17:32:16.166Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RandomSegmentationModel:\n",
    "    \"\"\"\n",
    "    Random segmentation model: \n",
    "        - generates random label maps for the inputs based on the class distributions observed during training\n",
    "        - every pixel in an input can only have one label\n",
    "    \"\"\"\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Adjusts the class ratio variable to the one observed in Y. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: list of arrays - n x (height x width x 3)\n",
    "        Y: list of arrays - n x (height x width)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        self.distribution = np.mean([[np.sum(Y_ == i) / Y_.size for i in range(len(labels) + 1)] for Y_ in Y], axis=0)\n",
    "        print(\"Setting class distribution to:\\nbackground: {}\\n{}\".format(self.distribution[0], \"\\n\".join(f\"{label}: {p}\" for label, p in zip(labels, self.distribution[1:]))))\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts for each input a label map.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: list of arrays - n x (height x width x 3)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Y_pred: list of arrays - n x (height x width)\n",
    "        \"\"\"\n",
    "        np.random.seed(0)\n",
    "        return [np.random.choice(np.arange(len(labels) + 1), size=X_.shape[:2], p=self.distribution) for X_ in X]\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)\n",
    "    \n",
    "model = RandomSegmentationModel()\n",
    "model.fit(train_df[\"img\"], train_df[\"seg\"])\n",
    "test_df.loc[:, \"seg\"] = model.predict(test_df[\"img\"])\n",
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic segmentation from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic segmentation involves classifying each pixel in an image into one of several predefined categories. It provides a dense, pixel-level understanding of the visual scene. \n",
    "\n",
    "Implemented U-Net allows to skip connections so features from the contracting path are concatenated with expanding path. Also it allows for symmetrical architecture featuring encoder-decoder structure with matching levels so all downsampling steps has a corresponding unsampling step. Additional Bottleneck Layer captures higher-level features before upsampling. It can provide better feature presentation because double convolutional blosck at each level help learning more robust features.\n",
    "\n",
    "The connection of traditional convolutional neural network with skip connections was used which helps with loosing resolution with feature extraction. Encoder has four layers were each of them has double convolutional layer 3x3 with batch normalisation and ReLu activation. ReLU is nonlinear activation function to help to learn more advanced fearures. Batch normalisation helps in data standarisation in one small batch so it has mean value of 0 and variance equal to 1. First layer of encoder has 64filters converting input image 126x128x3 every next doubles number of filters and at the same time it shrinks the spacial resolution. Bottelneck layer is the deeperst part of network and operates on the most abstract data. It  has the resolution of 16x16x512 and the double convolution 3x3 was used to get 1024 pieces. It connect the information from higher and lower level.  Decoder does the same thing as encoder but instead of max pooling it uses transposed convolution for upsampling. Each block starts with upsampling (up-trans) then concatenation is used with features in necoder. Two convolution 3x3 connects the connected features. The number of filters shrinks with the growing resolution. Skip connection halps with connection of encoder features with decoder so connect information about context with localisation.\n",
    "Output layer is 1x1 and transforms 64 channel of feature incon number of selected classes and generates probability map of each class in each piksel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "#Number of classes for segmentation including background class\n",
    "NUM_CLASSES = 21 #20 classes + background\n",
    "#target size (height, width) tp which all input images and masks will be resized - for the size consistancy in the network\n",
    "TARGET_SIZE = (128, 128)\n",
    "#Dataset\n",
    "#Define dataset for semantic segmentation - loading and preprocessin of image and mask pairs\n",
    "class SegmentationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Constructor for dataset \n",
    "    Initializes the dataset with a dataframe containing file paths or image/mask data,\n",
    "    also sets the target size for resizing and defines image transformations\n",
    "    \"\"\"\n",
    "    def __init__(self, df, target_size=(128, 128)):\n",
    "        #store the image data from dataframe\n",
    "        self.images = df[\"img\"].values\n",
    "        #store the mask data from dataframe\n",
    "        self.masks = df[\"seg\"].values\n",
    "        #store the target size for resizing\n",
    "        self.target_size = target_size\n",
    "        #Define image transformations: ToTensor converts the image to PyTorch Tensor\n",
    "        # It also scales pixel values from [0,255] to [0,1].\n",
    "        # Normalize the tensor with given mean and standard deviation which helps standarizing the input data distribution \n",
    "        self.img_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        \n",
    "    #return the total number of samples in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    #retrive the single sample at the given index\n",
    "    def __getitem__(self, idx):\n",
    "        #Load the image and mask data at the specified index and convert it to uint8 data type\n",
    "        img = self.images[idx].astype(np.uint8)\n",
    "        mask = self.masks[idx].astype(np.uint8)\n",
    "        #Apply the defined image transformations to the image\n",
    "        img = self.img_transform(img)\n",
    "        #Resize image to the target size\n",
    "        img = resize(img, self.target_size)\n",
    "        #Convert the mask numpy array to a PyTorch Tensor amd add a channel dimension using unsqueeze\n",
    "        #Convert to float initially, as resize expects float tensors\n",
    "        mask = torch.from_numpy(mask).unsqueeze(0).float()\n",
    "        #Resize the mask tensor to the target size\n",
    "        #Using InterpolationMode.NEAREST is crucial for masks to preserve discrete class labels\n",
    "        mask = resize(mask, self.target_size, interpolation=transforms.InterpolationMode.NEAREST)\n",
    "        #Remove the added channel dimension using squeeze\n",
    "        mask = mask.squeeze(0).long()\n",
    "\n",
    "        #Return the processed image and mask tensors\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, val_split=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split a dataframe into training and validation sets for evaluation\n",
    "    of the model performance on unseen data during training process\n",
    "\n",
    "    Use train_test_split from scikit-learn to perform the split.\n",
    "    df => The input dataframe\n",
    "    test_size => The proportion of the dataset to include in the validation split\n",
    "    random_state => Ensure reproducibility of the split\n",
    "    shuffle => Shuffle the data before splitting, important for preventing ordered biases\n",
    "    \"\"\"\n",
    "    train_df, val_df = train_test_split(df,test_size=val_split,\n",
    "        random_state=random_state, shuffle=True)\n",
    "    return train_df.reset_index(drop=True), val_df.reset_index(drop=True)\n",
    "\n",
    "#Split the main training dataframe into training and validation sets which provides data for training and evaluating the model during the training process\n",
    "train_df, val_df = split_dataframe(train_df)\n",
    "\n",
    "\"\"\"\n",
    "    Create dataset and dataloader for training and validation data\n",
    "    DataLoader provides an iterable over the dataset, handling batching, shuffling, and multiprocessing\n",
    "    batch_size => The number of samples per batch.\n",
    "    shuffle => Shuffle the data at each epoch\n",
    "    num_workers => Number of subprocesses to use for data loading(0 means the main process)\n",
    "\"\"\"\n",
    "train_dataset = SegmentationDataset(train_df, target_size=TARGET_SIZE)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = SegmentationDataset(val_df, target_size=TARGET_SIZE)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\" \n",
    "        Define of UNet model\n",
    "        It consists of a contracting path (encoder) to capture context and an expanding path (decoder)\n",
    "        to enable precise localization, with skip connections between the encoder and decoder\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes):\n",
    "        \"\"\"\n",
    "            Args: num_classes(init): number of ourput classes with background\n",
    "        \"\"\"\n",
    "        super().__init__() #construct the parent class nn.Module\n",
    "        self.num_classes = num_classes #store the number of classes\n",
    "        \n",
    "        \"\"\"\n",
    "            Contracting Path - Encoder\n",
    "            The network downsamples the input image and extracts features. \n",
    "            Each downsampling block consists of convolutional layers and a pooling layer and\n",
    "            the number of channels increases with depth to capture more complex features\n",
    "        \"\"\"\n",
    "        # First double convolution block: Input channels = 3 (for RGB images), Output channels = 64\n",
    "        self.down_conv1 = self.double_conv(3, 64)\n",
    "        # Second double convolution block: Input channels = 64, Output channels = 128\n",
    "        self.down_conv2 = self.double_conv(64, 128)\n",
    "        # Third double convolution block: Input channels = 128, Output channels = 256\n",
    "        self.down_conv3 = self.double_conv(128, 256)\n",
    "        # Fourth double convolution block: Input channels = 256, Output channels = 512\n",
    "        self.down_conv4 = self.double_conv(256, 512)\n",
    "        # Max pooling layer for downsampling => kernel size and stride of 2 reduce the spatial dimensions by half\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck - he layer with the lowest spatial resolution and highest number of channels connecting encoder and decoder\n",
    "        self.bottleneck = self.double_conv(512, 1024)\n",
    "        \n",
    "        \"\"\"\n",
    "            Expanding Path - Decoder\n",
    "            The network upsamples here the feature maps and reconstructs the segmentation mask.\n",
    "            It uses transposed convolutions (or upsampling followed by convolution) and skip connections.\n",
    "            The number of channels decreases with depth.\n",
    "        \"\"\"\n",
    "        # First transposed convolution for upsampling from the bottleneck\n",
    "        # Input channels = 1024, Output channels = 512. Kernel size and stride of 2 double the spatial dimensions.\n",
    "        self.up_trans1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        # First up-convolution block after the skip connection\n",
    "        # Input channels = 1024 (512 from transposed conv + 512 from skip connection), Output channels = 512\n",
    "        self.up_conv1 = self.double_conv(1024, 512)\n",
    "        # Second transposed convolution for upsampling\n",
    "        # Input channels = 512, Output channels = 256\n",
    "\n",
    "        self.up_trans2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        # Second up-convolution block after the skip connection\n",
    "        # Input channels = 512 (256 from transposed conv + 256 from skip connection), Output channels = 256\n",
    "        self.up_conv2 = self.double_conv(512, 256)\n",
    "\n",
    "        # Third transposed convolution for upsampling\n",
    "        # Input channels = 256, Output channels = 128\n",
    "        self.up_trans3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        # Third up-convolution block after the skip connection\n",
    "        # Input channels = 256 (128 from transposed conv + 128 from skip connection), Output channels = 128\n",
    "        self.up_conv3 = self.double_conv(256, 128)\n",
    "\n",
    "        # Fourth transposed convolution for upsampling\n",
    "        # Input channels = 128, Output channels = 64\n",
    "        self.up_trans4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        # Fourth up-convolution block after the skip connection\n",
    "        # Input channels = 128 (64 from transposed conv + 64 from skip connection), Output channels = 64\n",
    "        self.up_conv4 = self.double_conv(128, 64)\n",
    "        \n",
    "        # Final output layer\n",
    "        # A 1x1 convolution to map the final feature maps to the number of classes\n",
    "        # Input channels = 64, Output channels = num_classes\n",
    "        self.out_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "    \n",
    "    #Double convolutional block function  Consists of two convolutional layers, each followed by batch normalization and ReLU activation\n",
    "    def double_conv(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "            Double convolution block: Conv -> BatchNorm -> ReLU -> Conv -> BatchNorm -> ReLU\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            # First convolutional layer. Kernel size 3x3, padding 1 to maintain spatial dimensions\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            # Batch normalization layer to normalize the activations, improving training stability\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            # ReLU activation function for non-linearity. inplace=True saves memory\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Second convolutional layer\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            # Second batch normalization layer\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            # Second ReLU activation function\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Defines the forward pass of the U-Net model\n",
    "            Args:\n",
    "                x: The input tensor (image batch)\n",
    "        \"\"\"\n",
    "        # Forward pass through Encoder\n",
    "        # Apply the first double convolution block. Store the output (x1) for the skip connection\n",
    "        x1 = self.down_conv1(x)\n",
    "        # Apply max pooling to reduce spatial dimensions\n",
    "        x2 = self.maxpool(x1)\n",
    "        \n",
    "        # Apply the second double convolution block. Store the output (x3) for the skip connection\n",
    "        x3 = self.down_conv2(x2) \n",
    "        # Apply max pooling\n",
    "        x4 = self.maxpool(x3)\n",
    "        \n",
    "        # Apply the third double convolution block. Store the output (x5) for the skip connection\n",
    "        x5 = self.down_conv3(x4)\n",
    "        # Apply max pooling\n",
    "        x6 = self.maxpool(x5)\n",
    "        \n",
    "        # Apply the fourth double convolution block. Store the output (x7) for the skip connection\n",
    "        x7 = self.down_conv4(x6)\n",
    "        # Apply max pooling => the input to the bottleneck\n",
    "        x8 = self.maxpool(x7)\n",
    "        \n",
    "        # Bottleneck - Apply the bottleneck double convolution block\n",
    "        x9 = self.bottleneck(x8)\n",
    "        \n",
    "        # Forward pass through the Decoder \n",
    "        # Apply the first transposed convolution to upsample from the bottleneck\n",
    "        x = self.up_trans1(x9)\n",
    "        # Concatenate the upsampled feature map with the corresponding feature map from the encoder (x7)\n",
    "        # This is the skip connection, providing high-resolution features to the decoder.\n",
    "        # dim=1 means concatenating along the channel dimension\n",
    "        x = torch.cat([x, x7], dim=1)  # Skip connection\n",
    "        # Apply the first up-convolution block\n",
    "        x = self.up_conv1(x)\n",
    "        \n",
    "        # Apply the second transposed convolution\n",
    "        x = self.up_trans2(x)\n",
    "        # Concatenate with the feature map from the encoder (x5)\n",
    "        x = torch.cat([x, x5], dim=1)  # Skip connection\n",
    "        # Apply the second up-convolution block\n",
    "        x = self.up_conv2(x)\n",
    "        \n",
    "        # Apply the third transposed convolution\n",
    "        x = self.up_trans3(x)\n",
    "        # Concatenate with the feature map from the encoder (x3)\n",
    "        x = torch.cat([x, x3], dim=1)  # Skip connection\n",
    "        # Apply the third up-convolution block\n",
    "        x = self.up_conv3(x)\n",
    "        \n",
    "        # Apply the fourth transposed convolution\n",
    "        x = self.up_trans4(x)\n",
    "        # Concatenate with the feature map from the encoder (x1)\n",
    "        x = torch.cat([x, x1], dim=1)  # Skip connection\n",
    "        # Apply the fourth up-convolution block\n",
    "        x = self.up_conv4(x)\n",
    "        \n",
    "        # Final output => Apply the 1x1 convolution to produce the final segmentation map\n",
    "        out = self.out_conv(x)\n",
    "        #Return the output tensor \n",
    "        return out\n",
    "\n",
    "#Set device for training (GPU if available otherwise use CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#Initialize U-Net model and move it to the selected device\n",
    "model = UNet(NUM_CLASSES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Used as loss function especially when there is class imbalance\n",
    "    It measures the similarity between the predicted segmentation and the ground truth mask\n",
    "    Args:\n",
    "        smooth => A small value added to the numerator and denominator to prevent division by zero\n",
    "        ignore_index => Class index to ignore in the loss calculation for example invalid regions\n",
    "    \"\"\"\n",
    "    def __init__(self, smooth=1, ignore_index=255):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "        self.smooth = smooth # Store the smoothing value\n",
    "        self.ignore_index = ignore_index # Store the index to ignore\n",
    "\n",
    "    \"\"\"\n",
    "        Forward pass for the Dice Loss calculation\n",
    "        Args:\n",
    "            pred => The predicted segmentation map\n",
    "            target=> The ground truth segmentation mask\n",
    "    \"\"\"\n",
    "    def forward(self, pred, target):\n",
    "        # Apply softmax to the predicted logits to get probabilities for each class\n",
    "        # dim=1 means applying softmax across the channel dimension\n",
    "        pred = torch.softmax(pred, dim=1)\n",
    "        # Get the number of classes from the prediction tensor\n",
    "        num_classes = pred.shape[1]\n",
    "        \n",
    "        # Create a mask to exclude pixels with the ignore_index from the loss calculation\n",
    "        mask = (target != self.ignore_index).float()\n",
    "        #Apply the mask to the target, convert back to long as target should have class indices\n",
    "        target = target * mask.long()\n",
    "        \n",
    "        # Convert the target mask to one-hot encoding\n",
    "        #Create a binary tensor where for each pixel, only the channel corresponding to the\n",
    "        # ground truth class is 1, and others are 0\n",
    "        target_onehot = torch.nn.functional.one_hot(target, num_classes=num_classes).permute(0,3,1,2)\n",
    "        \n",
    "        # Calculate the intersection between the predicted probabilities and the one-hot target\n",
    "        # Sum across the spatial dimensions (height and width) to get the intersection for each class in each batch\n",
    "        intersection = (pred * target_onehot).sum(dim=(2,3))\n",
    "        # Calculate the union of the predicted probabilities and the one-hot target and sum across the spatial dimensions\n",
    "        union = pred.sum(dim=(2,3)) + target_onehot.sum(dim=(2,3))\n",
    "        # Calculate the Dice coefficient for each class in each batch\n",
    "        # Add 'smooth' to numerator and denominator to avoid division by zero\n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "\n",
    "        # Return the mean Dice loss (1 - Dice coefficient) averaged across all classes and batches.\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "\"\"\"\"\n",
    "Define a combined loss function that is a weighted sum of Cross-Entropy Loss and Dice Loss.\n",
    "Using a combination of loss functions can often lead to better performance, especially\n",
    "for segmentation tasks with class imbalance. Cross-Entropy focuses on individual pixel classification,\n",
    "while Dice Loss focuses on the overall overlap of segmentation regions\n",
    "\"\"\"\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        weight => Class weights for Cross-Entropy Loss to handle class imbalance\n",
    "        alpha => Weighting factor for the Cross-Entropy Loss (1 - alpha is the weight for Dice Loss)\n",
    "    \"\"\"\n",
    "    def __init__(self, weight=None, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        # Initialize the Cross-Entropy Loss\n",
    "        self.ce_loss = nn.CrossEntropyLoss(weight=weight)\n",
    "        # Initialize the Dice Loss\n",
    "        self.dice_loss = DiceLoss()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"\n",
    "        Define the forward pass for the Combined Loss calculation\n",
    "        pred => The predicted segmentation map \n",
    "        target => The ground truth segmentation mask\n",
    "        \"\"\"\n",
    "        # Calculate the Cross-Entropy Loss\n",
    "        ce = self.ce_loss(pred, target)\n",
    "        # Calculate the Dice Loss\n",
    "        dice = self.dice_loss(pred, target)\n",
    "        # Return the weighted sum of the two losses\n",
    "        return self.alpha * ce + (1 - self.alpha) * dice\n",
    "    \n",
    "\"\"\"\n",
    "    Class weighting for impalanced datasets\n",
    "    Class weighting assigns higher importance to less frequent classes during training,\n",
    "    helping the model learn to segment them better\n",
    "\"\"\"\n",
    "# Calculate the count of pixels for each class in the training masks\n",
    "# np.concatenate joins all mask arrays into a single array,\n",
    "# np.bincount counts the occurrences of each non-negative integer value\n",
    "class_counts = np.bincount(np.concatenate([m.flatten() for m in train_df[\"seg\"]]))\n",
    "# Calculate initial class weights as the inverse of class counts\n",
    "class_weights = 1. / torch.tensor(class_counts, dtype=torch.float32)\n",
    "# Normalize the class weights so they sum to 1, ensurING that the overall scale of the weighted loss is consistent\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "# Initialize the combined loss function with calculated class weights and an alpha value\n",
    "criterion = CombinedLoss(weight=class_weights.to(device), alpha=0.5)\n",
    "\n",
    "\"\"\"\n",
    "    Adam optimization algorithm  adapts the learning rate for each parameter\n",
    "    Args:\n",
    "        model.parameters()=> specifies which parameters of the model should be optimized\n",
    "        lr => learning rate, controls the step size during optimization\n",
    "        weight_decay => L2 regularization term, helps prevent overfitting by penalizing large weights\n",
    "\"\"\"\n",
    "#Selection optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "# Choose a learning rate scheduler to adjust the learning rate during training\n",
    "# 'min' =>  Monitor a metric that should be minimized (validation loss)\n",
    "# patience => Number of epochs with no improvement after which the learning rate will be reduced\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vizualise train losses\n",
    "# Lists to store training and validation losses for plotting\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Number of epochs to train the model => An epoch is one full pass through the training dataset\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    # Initialize running loss for the current epoch\n",
    "    running_loss = 0.0\n",
    "    # Iterate over batches in the training data loader\n",
    "    for imgs, masks in train_loader:\n",
    "        # Move images and masks to the selected device\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        \n",
    "        # Zero the gradients of the model parameters\n",
    "        # Gradients are accumulated by default, so this is necessary to prevent\n",
    "        # gradients from previous iterations affecting the current update\n",
    "        optimizer.zero_grad()\n",
    "        # Perform a forward pass to get model predictions for the current batch of images\n",
    "        outputs = model(imgs) \n",
    "        # Calculate the loss using the defined criterion and the predictions and ground truth masks\n",
    "        loss = criterion(outputs, masks)\n",
    "        # Perform backpropagation - calculate gradients of the loss with respect to the model parameters\n",
    "        loss.backward()\n",
    "        # Update the model parameters using the calculated gradients and the optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the loss for the current epoch\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss/len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    # Set the model to evaluation mode.\n",
    "    model.eval()\n",
    "    # Initialize validation loss for the current epoch\n",
    "    val_loss = 0.0\n",
    "    # Disable gradient calculation during validation\n",
    "    with torch.no_grad():\n",
    "        # Iterate over batches in the validation data loader\n",
    "        for imgs, masks in val_loader:\n",
    "            # Move images and masks to the selected device\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            # Perform a forward pass to get model predictions\n",
    "            outputs = model(imgs)\n",
    "            # Calculate the loss on the validation data\n",
    "            loss = criterion(outputs, masks)\n",
    "            # Accumulate the validation loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # Calculate the average validation loss for the epoch\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.4f} - Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Plot the training loss over epochs\n",
    "plt.plot(range(1, num_epochs+1), train_losses, 'b-o', label='Training Loss', linewidth=2, markersize=8)\n",
    "# Plot the validation loss over epochs\n",
    "plt.plot(range(1, num_epochs+1), val_losses, 'r-o', label='Validation Loss',linewidth=2, markersize=8)\n",
    "plt.title('Training and Validation Loss over Epochs', fontsize=14)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.xticks(range(1, num_epochs+1))\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the test results\n",
    "def show_predictions(model, dataloader, num_show):\n",
    "    # Set the model to evaluation mode.\n",
    "    model.eval()\n",
    "    imgs, masks = next(iter(dataloader))\n",
    "    imgs, masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "    # Disable gradient calculation for predictions\n",
    "    with torch.no_grad():\n",
    "        preds = model(imgs)\n",
    "        preds = torch.argmax(preds, dim=1)\n",
    "\n",
    "    #Convert to numpy for visualization\n",
    "    imgs_np = imgs.cpu().numpy()\n",
    "    masks_np = masks.cpu().numpy()\n",
    "    preds_np = preds.cpu().numpy()\n",
    "\n",
    "    # Reverse normalization applied during preprocessing to display the images correctly\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    imgs_np = imgs_np.transpose(0, 2, 3, 1)\n",
    "    imgs_np = imgs_np * std + mean\n",
    "    imgs_np = np.clip(imgs_np, 0, 1)\n",
    "\n",
    "    #Plot results\n",
    "    num_show = min(3, len(imgs))\n",
    "    _, axs = plt.subplots(num_show, 3, figsize=(15, 5*num_show))\n",
    "\n",
    "    # Iterate through the selected number of samples\n",
    "    for i in range(num_show):\n",
    "        axs[i, 0].imshow(imgs_np[i])\n",
    "        axs[i, 0].set_title(\"Input Image\")\n",
    "        axs[i, 0].axis('off')\n",
    "        \n",
    "        axs[i, 1].imshow(masks_np[i], vmin=0, vmax=NUM_CLASSES-1, cmap='jet')\n",
    "        axs[i, 1].set_title(\"Ground Truth\")\n",
    "        axs[i, 1].axis('off')\n",
    "        \n",
    "        axs[i, 2].imshow(preds_np[i], vmin=0, vmax=NUM_CLASSES-1, cmap='jet')\n",
    "        axs[i, 2].set_title(\"Prediction\")\n",
    "        axs[i, 2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_predictions(model, val_loader, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models.segmentation as models\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "import matplotlib.colors as mcolors\n",
    "import pydensecrf.densecrf as dcrf\n",
    "from pydensecrf.utils import unary_from_softmax, create_pairwise_bilateral\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 0\n",
    "EPOCH = 70\n",
    "N_frozen = 3\n",
    "LR = 1e-5\n",
    "LR_FROZEN = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    elif hasattr(torch, 'xla') and torch.xla.device_count() > 0:\n",
    "        return torch.device('xla')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the training data\n",
    "train_df = pd.read_csv('data/train/train_set.csv', index_col=\"Id\")\n",
    "labels = train_df.columns\n",
    "train_df[\"img\"] = [np.load('data/train/img/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n",
    "train_df[\"seg\"] = [np.load('data/train/seg/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n",
    "print(\"The training set contains {} examples.\".format(len(train_df)))\n",
    "\n",
    "# Show some examples\n",
    "fig, axs = plt.subplots(2, 20, figsize=(10 * 20, 10 * 2))\n",
    "for i, label in enumerate(labels):\n",
    "    df = train_df.loc[train_df[label] == 1]\n",
    "    axs[0, i].imshow(df.iloc[0][\"img\"], vmin=0, vmax=255)\n",
    "    axs[0, i].set_title(\"\\n\".join(label for label in labels if df.iloc[0][label] == 1), fontsize=40)\n",
    "    axs[0, i].axis(\"off\")\n",
    "    axs[1, i].imshow(df.iloc[0][\"seg\"], vmin=0, vmax=20)  # with the absolute color scale it will be clear that the arrays in the \"seg\" column are label maps (labels in [0, 20])\n",
    "    axs[1, i].axis(\"off\")\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# The training dataframe contains for each image 20 columns with the ground truth classification labels and 20 column with the ground truth segmentation maps for each class\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the test data\n",
    "test_df = pd.read_csv('data/test/test_set.csv', index_col=\"Id\")\n",
    "test_df[\"img\"] = [np.load('data/test/img/test_{}.npy'.format(idx)) for idx, _ in test_df.iterrows()]\n",
    "test_df[\"seg\"] = [-1 * np.ones(img.shape[:2], dtype=np.int8) for img in test_df[\"img\"]]\n",
    "print(\"The test set contains {} examples.\".format(len(test_df)))\n",
    "\n",
    "# The test dataframe is similar to the training dataframe, but here the values are -1 --> your task is to fill in these as good as possible in Sect. 2 and Sect. 3; in Sect. 6 this dataframe is automatically transformed in the submission CSV!\n",
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOC2009Dataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None, target_transform=None, paired_transform=None, ignore_label=21):\n",
    "        self.df = dataframe.reset_index()\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.paired_transform = paired_transform\n",
    "        \n",
    "        self.label_columns = [col for col in self.df.columns if col not in ['img', 'seg', 'Id']]\n",
    "        self.ignore_label = ignore_label\n",
    "        self.classes = 22  # 20 classes + background + void\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.df.iloc[idx]['img'] \n",
    "        mask = self.df.iloc[idx]['seg']   \n",
    "\n",
    "        image = Image.fromarray(image.astype(np.uint8))  \n",
    "        mask = Image.fromarray(mask.astype(np.uint8))    \n",
    "\n",
    "        if self.paired_transform:\n",
    "            image, mask = self.paired_transform(image, mask)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            mask = self.target_transform(mask)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_transform_aug = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(\n",
    "        shift_limit=0.05,     \n",
    "        scale_limit=0.10,     \n",
    "        rotate_limit=15,      \n",
    "        interpolation=1,\n",
    "        p=0.5\n",
    "    ),\n",
    "\n",
    "    A.Resize(256, 256, interpolation=0),\n",
    "],\n",
    "additional_targets={'mask': 'mask'}\n",
    ")\n",
    "\n",
    "paired_transform = A.Compose([\n",
    "    A.Resize(256, 256, interpolation=1),\n",
    "],\n",
    "additional_targets={'mask': 'mask'}\n",
    ")\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: torch.tensor(np.array(x), dtype=torch.long)),\n",
    "    transforms.Lambda(lambda x: torch.where(x == 255, 21, x))\n",
    "])\n",
    "\n",
    "def apply_paired_transform(image, mask):\n",
    "    image_np = np.array(image)\n",
    "    mask_np = np.array(mask)\n",
    "    \n",
    "    augmented = paired_transform(image=image_np, mask=mask_np)\n",
    "    \n",
    "    image_aug = Image.fromarray(augmented['image'])\n",
    "    mask_aug = Image.fromarray(augmented['mask'])\n",
    "    \n",
    "    return image_aug, mask_aug\n",
    "\n",
    "def apply_paired_transform_aug(image, mask):\n",
    "    image_np = np.array(image)\n",
    "    mask_np = np.array(mask)\n",
    "    \n",
    "    augmented = paired_transform_aug(image=image_np, mask=mask_np)\n",
    "    \n",
    "    image_aug = Image.fromarray(augmented['image'])\n",
    "    mask_aug = Image.fromarray(augmented['mask'])\n",
    "    \n",
    "    return image_aug, mask_aug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, val_split=0.2, random_state=42):\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    train_df, val_df = train_test_split(\n",
    "        df,\n",
    "        test_size=val_split,\n",
    "        random_state=random_state,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    val_df = val_df.reset_index(drop=True)\n",
    "    \n",
    "    return train_df, val_df\n",
    "\n",
    "train_df, val_df = split_dataframe(train_df)\n",
    "\n",
    "train_aug_dataset = VOC2009Dataset(\n",
    "    dataframe=train_df,\n",
    "    transform=image_transform,\n",
    "    target_transform=mask_transform,\n",
    "    paired_transform=apply_paired_transform_aug\n",
    "    )\n",
    "# train_ori_dataset =  VOC2009Dataset(\n",
    "#     dataframe=train_df,\n",
    "#     transform=image_transform,\n",
    "#     target_transform=mask_transform,\n",
    "#     paired_transform=apply_paired_transform\n",
    "#     )\n",
    "\n",
    "# train_dataset = ConcatDataset([train_aug_dataset, train_ori_dataset])\n",
    "train_dataloader = DataLoader(train_aug_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "val_dataset = VOC2009Dataset(\n",
    "    dataframe=val_df,\n",
    "    transform=image_transform,\n",
    "    target_transform=mask_transform,\n",
    "    paired_transform=apply_paired_transform\n",
    ")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=1e-4, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.delta = delta # Minimum improvement\n",
    "        self.verbose = verbose\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss  # Convert to negative if minimizing loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter}/{self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.4f} --> {val_loss:.4f}). Saving model...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.best_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1, ignore_index=21, from_logits=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.ignore_index = ignore_index\n",
    "        self.from_logits = from_logits\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        if self.from_logits:\n",
    "            pred = torch.softmax(pred, dim=1) \n",
    "            \n",
    "        num_classes = pred.size(1) + 1\n",
    "\n",
    "        mask = (target != self.ignore_index).float()\n",
    "        \n",
    "        target = torch.nn.functional.one_hot(target.long(), num_classes=num_classes)  # [batch_size, height, width, num_classes]\n",
    "        target = target.permute(0, 3, 1, 2).float()  # [batch_size, num_classes, height, width]\n",
    "        \n",
    "        # Apply mask to target\n",
    "        mask_target = mask.unsqueeze(1).expand_as(target)  # [batch_size, 22, height, width]\n",
    "        target = target * mask_target  # Zero out ignored pixels\n",
    "        target = target[:, :-1] # [batch_size, 21, height, width]\n",
    "        \n",
    "        # Apply mask to predictions\n",
    "        mask_pred = mask.unsqueeze(1).expand_as(pred) # [batch_size, 21, height, width]\n",
    "        pred = pred * mask_pred \n",
    "\n",
    "        # Flatten predictions and targets for each class\n",
    "        pred = pred.contiguous().view(-1, pred.size(1))  # [batch_size * height * width, num_classes]\n",
    "        target = target.contiguous().view(-1, target.size(1))  # [batch_size * height * width, num_classes]\n",
    "        \n",
    "        # Compute Dice coefficient for each class\n",
    "        intersection = (pred * target).sum(dim=0)  # Sum over pixels for each class\n",
    "        union = pred.sum(dim=0) + target.sum(dim=0)  # Sum over pixels for each class ans substract the intersection\n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth + 1e-8)  # Dice score per class\n",
    "        \n",
    "        # Return 1 - mean Dice score as loss\n",
    "        return 1 - dice.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedCEDiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1, ignore_index=21, alpha=0.5):\n",
    "        super(WeightedCEDiceLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.diceloss_fn = DiceLoss(smooth, ignore_index)\n",
    "        self.wceloss_fn = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        diceloss = self.diceloss_fn.forward(pred, target)\n",
    "        wceloss = self.wceloss_fn.forward(pred, target)\n",
    "        return self.alpha * diceloss + (1 - self.alpha) * wceloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.deeplabv3_resnet50(pretrained=True, num_classes=21) \n",
    "criterion = WeightedCEDiceLoss(alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "for m in model.backbone.modules():\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        m.eval()\n",
    "\n",
    "head_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': head_params, 'lr': LR_FROZEN},\n",
    "], lr=LR_FROZEN)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = EPOCH\n",
    "model = model.to(device)\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with tqdm(train_dataloader, desc=f\"Training epoch [{epoch+1}/{num_epochs}]\", unit=\"batch\") as pbar:\n",
    "        for images, masks in pbar:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(images)['out']\n",
    "            loss = criterion(logits, masks)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_dataloader):.4f}\")\n",
    "        train_losses.append(running_loss/len(train_dataloader))\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(val_dataloader, desc=f\"Validating\", unit=\"batch\") as pbar:\n",
    "            for images, masks in pbar:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                logits = model(images)['out']\n",
    "                loss = criterion(logits, masks)\n",
    "                val_loss += loss.item()\n",
    "    \n",
    "    val_loss = val_loss / len(val_dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Val Loss: {val_loss:.4f}\")\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    early_stopping(val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "    \n",
    "    if epoch == N_frozen:\n",
    "        for param in model.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "        for m in model.backbone.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.train()\n",
    "\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params': model.backbone.parameters(), 'lr': LR},   \n",
    "            {'params': model.classifier.parameters(), 'lr': LR},\n",
    "        ])\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crf_refine_logits(logits, img, n_iters: int = 5):\n",
    "    probs = torch.softmax(logits, dim=0).cpu().numpy()\n",
    "    C, H, W = probs.shape\n",
    "    U = unary_from_softmax(probs)\n",
    "    d = dcrf.DenseCRF2D(W, H, C)\n",
    "    d.setUnaryEnergy(U)\n",
    "    \n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], device=img.device)[:, None, None]\n",
    "    std  = torch.tensor([0.229, 0.224, 0.225], device=img.device)[:, None, None]\n",
    "\n",
    "    img_unnorm = img * std + mean  \n",
    "\n",
    "    img_uint8 = (img_unnorm.clamp(0,1) * 255).byte()  \n",
    "    img_np = img_uint8.permute(1, 2, 0).cpu().numpy()  \n",
    "\n",
    "    feats = create_pairwise_bilateral(\n",
    "        sdims=(20, 20), schan=(13,13,13),\n",
    "        img=img_np, chdim=2\n",
    "    )\n",
    "    d.addPairwiseEnergy(feats, compat=21)\n",
    "\n",
    "    Q = d.inference(n_iters)                     \n",
    "\n",
    "    refined_probs = np.array(Q).reshape((C, H, W))\n",
    "    return refined_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def binary_masks_from_logits(logits, thresholds, img, refine_logits=True):\n",
    "#     if refine_logits:\n",
    "#         probs = crf_refine_logits(logits, img)\n",
    "#     else:\n",
    "#         probs = torch.softmax(logits, dim=0).cpu().numpy() \n",
    "\n",
    "#     C, H, W = probs.shape\n",
    "\n",
    "#     thr = np.array([thresholds.get(c, 0.5) for c in range(C)], dtype=np.float32)\n",
    "#     thr = thr[:, None, None]                              \n",
    "\n",
    "#     passed = probs > thr                                 \n",
    "#     masked_probs = probs * passed.astype(np.float32)      \n",
    "#     label_map = masked_probs.argmax(axis=0)             \n",
    "#     masks = np.zeros_like(passed, dtype=np.uint8)\n",
    "#     for c in range(C):\n",
    "#         masks[c] = (label_map == c).astype(np.uint8)\n",
    "\n",
    "#     return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def post_process(logits_batch, imgs_batch, thresholds):\n",
    "#     processed_predicts_batch = []\n",
    "#     for (logits, img) in zip(logits_batch, imgs_batch):\n",
    "#         processed_predicts = binary_masks_from_logits(logits, thresholds, img)\n",
    "#         processed_predicts_batch.append(processed_predicts)\n",
    "\n",
    "#     processed_predicts_batch = torch.tensor(processed_predicts_batch, dtype=torch.float32)\n",
    "#     return processed_predicts_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(logits_batch, imgs_batch):\n",
    "    processed_logits_batch = []\n",
    "    for (logits, img) in zip(logits_batch, imgs_batch):\n",
    "        processed_logits = crf_refine_logits(logits, img)\n",
    "        processed_logits_batch.append(processed_logits)\n",
    "\n",
    "    processed_logits_batch = torch.tensor(processed_logits_batch, dtype=torch.float32)\n",
    "    return processed_logits_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_loss = DiceLoss(smooth=1, from_logits=False)\n",
    "    \n",
    "model.eval()\n",
    "total_loss = 0.0\n",
    "num_batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    with tqdm(val_dataloader, desc=f\"Validating\", unit=\"batch\") as pbar:\n",
    "        for images, masks in pbar:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            logits = model(images)['out']\n",
    "            # processed_predicts = post_process(logits, images, thresholds)\n",
    "            # processed_predicts = processed_predicts.to(device)\n",
    "            logits = post_process(logits, images)\n",
    "            logits = logits.to(device)\n",
    "            loss = dice_loss(logits, masks)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "avg_loss = total_loss / num_batches\n",
    "print(f'Final average DICE score: {1 - avg_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_segmentation(image, mask, pred):\n",
    "    # Convert tensors to NumPy\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()  # Convert to HWC\n",
    "    mask = mask.cpu().numpy()\n",
    "    pred = pred.cpu().numpy()\n",
    "    \n",
    "    # Ensure mask and pred are 2D (H, W)\n",
    "    if mask.ndim > 2:\n",
    "        mask = mask.squeeze()\n",
    "    if pred.ndim > 2:\n",
    "        pred = pred.squeeze()\n",
    "    \n",
    "    # Initialize RGB images for masks\n",
    "    height, width = mask.shape\n",
    "    colored_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    colored_pred = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Get the viridis colormap\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    norm = mcolors.Normalize(vmin=0, vmax=20)  # Scale for labels [0, 20]\n",
    "    \n",
    "    # Map class indices to colors for ground truth and prediction\n",
    "    for class_idx in np.unique(np.concatenate([mask, pred])):\n",
    "        if class_idx <= 20:\n",
    "            # Convert normalized colormap value to RGB (0-255)\n",
    "            color = cmap(norm(class_idx))[:3]  # Get RGB (ignore alpha)\n",
    "            color = (np.array(color) * 255).astype(np.uint8)\n",
    "            colored_mask[mask == class_idx] = color\n",
    "            colored_pred[pred == class_idx] = color\n",
    "        elif class_idx == 255:\n",
    "            # Void label mapped to white, consistent with original visualize_segmentation\n",
    "            colored_mask[mask == class_idx] = (255, 255, 255)\n",
    "            colored_pred[pred == class_idx] = (255, 255, 255)\n",
    "        else:\n",
    "            print(f\"Warning: Class index {class_idx} not in expected range [0, 20] or 255. Using black.\")\n",
    "            colored_mask[mask == class_idx] = (0, 0, 0)\n",
    "            colored_pred[pred == class_idx] = (0, 0, 0)\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"Input Image\")\n",
    "    plt.imshow(image)  # May need denormalization if normalized\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"Ground Truth\")\n",
    "    plt.imshow(colored_mask)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Prediction\")\n",
    "    plt.imshow(colored_pred)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Return colored masks as PIL Images\n",
    "    return Image.fromarray(colored_mask), Image.fromarray(colored_pred)\n",
    "\n",
    "# Example visualization\n",
    "images, masks = next(iter(val_dataloader))\n",
    "images = images.to(device)\n",
    "masks = masks.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)['out']\n",
    "    preds = post_process(logits, images)\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "    preds = torch.where(preds == 21, 255, preds)\n",
    "\n",
    "visualize_segmentation(images[1], masks[1], preds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss(train_loss, val_loss, save_path=None):\n",
    "    epochs = list(range(1, len(train_loss) + 1))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_loss, label='Training Loss', marker='o', color='blue')\n",
    "    plt.plot(epochs, val_loss, label='Validation Loss', marker='s', color='orange')\n",
    "    \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (DICE)')\n",
    "    plt.title('Training and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Plot saved to {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "visualize_loss(train_losses, val_losses, 'loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit to competition\n",
    "You don't need to edit this section. Just use it at the right position in the notebook. See the definition of this function in Sect. 1.3 for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-07T17:32:16.166Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "generate_submission(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Adversarial attack\n",
    "For this part, your goal is to fool your classification and/or segmentation CNN, using an *adversarial attack*. More specifically, the goal is build a CNN to perturb test images in a way that (i) they look unperturbed to humans; but (ii) the CNN classifies/segments these images in line with the perturbations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Discussion\n",
    "Finally, take some time to reflect on what you have learned during this assignment. Reflect and produce an overall discussion with links to the lectures and \"real world\" computer vision."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11236659,
     "sourceId": 94526,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
